{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.types import DoubleType, ArrayType, FloatType\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "from typing import Iterator, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "sparkInit = SparkSession.builder \\\n",
    "            .appName(\"TransactionClassificationGNN\") \\\n",
    "            .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "            .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "            .config(\"spark.sql.debug.maxToStringFields\", \"100\") \\\n",
    "            .getOrCreate()\n",
    "# def create_spark_session():\n",
    "#     \"\"\"\n",
    "#     Create a Spark session with\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         return SparkSession.builder \\\n",
    "#             .appName(\"TransactionClassificationGNN\") \\\n",
    "#             .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "#             .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "#             .config(\"spark.sql.debug.maxToStringFields\", \"100\") \\\n",
    "#             .getOrCreate()\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error creating Spark session: {e}\")\n",
    "#         raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "SparkContext or SparkSession should be created first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mImprovedSparkGNN\u001b[39;00m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_dir, num_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m166\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;66;03m# Create Spark session with robust configuration\u001b[39;00m\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder \\\n\u001b[1;32m      5\u001b[0m             \u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransaction_Classification_GNN\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      6\u001b[0m             \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.adaptive.enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      7\u001b[0m             \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.dynamicAllocation.enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      8\u001b[0m             \u001b[38;5;241m.\u001b[39mgetOrCreate()\n",
      "Cell \u001b[0;32mIn[16], line 98\u001b[0m, in \u001b[0;36mImprovedSparkGNN\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;129;43m@pandas_udf\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marray<float>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPandasUDFType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGROUPED_MAP\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 98\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mfeature_engineering_udf\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpdf\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"\u001b[39;49;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;43;03m    Pandas UDF for feature engineering and normalization\u001b[39;49;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;43;03m    \"\"\"\u001b[39;49;00m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Dynamically determine feature columns\u001b[39;49;00m\n",
      "File \u001b[0;32m/home/vuphamas/code/fx/venv/lib/python3.10/site-packages/pyspark/sql/pandas/functions.py:480\u001b[0m, in \u001b[0;36m_create_pandas_udf\u001b[0;34m(f, returnType, evalType)\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _create_connect_udf(f, returnType, evalType)\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_create_udf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturnType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalType\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/vuphamas/code/fx/venv/lib/python3.10/site-packages/pyspark/sql/udf.py:84\u001b[0m, in \u001b[0;36m_create_udf\u001b[0;34m(f, returnType, evalType, name, deterministic)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Set the name of the UserDefinedFunction object to be the name of function f\u001b[39;00m\n\u001b[1;32m     81\u001b[0m udf_obj \u001b[38;5;241m=\u001b[39m UserDefinedFunction(\n\u001b[1;32m     82\u001b[0m     f, returnType\u001b[38;5;241m=\u001b[39mreturnType, name\u001b[38;5;241m=\u001b[39mname, evalType\u001b[38;5;241m=\u001b[39mevalType, deterministic\u001b[38;5;241m=\u001b[39mdeterministic\n\u001b[1;32m     83\u001b[0m )\n\u001b[0;32m---> 84\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mudf_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/vuphamas/code/fx/venv/lib/python3.10/site-packages/pyspark/sql/udf.py:433\u001b[0m, in \u001b[0;36mUserDefinedFunction._wrapped\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    426\u001b[0m wrapper\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__module__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\n\u001b[1;32m    430\u001b[0m )\n\u001b[1;32m    432\u001b[0m wrapper\u001b[38;5;241m.\u001b[39mfunc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m--> 433\u001b[0m wrapper\u001b[38;5;241m.\u001b[39mreturnType \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturnType\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    434\u001b[0m wrapper\u001b[38;5;241m.\u001b[39mevalType \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevalType  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    435\u001b[0m wrapper\u001b[38;5;241m.\u001b[39mdeterministic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeterministic  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
      "File \u001b[0;32m/home/vuphamas/code/fx/venv/lib/python3.10/site-packages/pyspark/sql/udf.py:214\u001b[0m, in \u001b[0;36mUserDefinedFunction.returnType\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_returnType_placeholder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_returnType\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 214\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_returnType_placeholder \u001b[38;5;241m=\u001b[39m \u001b[43m_parse_datatype_string\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_returnType\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevalType \u001b[38;5;241m==\u001b[39m PythonEvalType\u001b[38;5;241m.\u001b[39mSQL_SCALAR_PANDAS_UDF\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevalType \u001b[38;5;241m==\u001b[39m PythonEvalType\u001b[38;5;241m.\u001b[39mSQL_SCALAR_PANDAS_ITER_UDF\n\u001b[1;32m    219\u001b[0m ):\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/home/vuphamas/code/fx/venv/lib/python3.10/site-packages/pyspark/sql/types.py:1293\u001b[0m, in \u001b[0;36m_parse_datatype_string\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m   1249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_parse_datatype_string\u001b[39m(s: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataType:\n\u001b[1;32m   1250\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1251\u001b[0m \u001b[38;5;124;03m    Parses the given data type string to a :class:`DataType`. The data type string format equals\u001b[39;00m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;124;03m    :class:`DataType.simpleString`, except that the top level struct type can omit\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;124;03m    ParseException:...\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1293\u001b[0m     sc \u001b[38;5;241m=\u001b[39m \u001b[43mget_active_spark_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1295\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_ddl_schema\u001b[39m(type_str: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataType:\n\u001b[1;32m   1296\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _parse_datatype_json_string(\n\u001b[1;32m   1297\u001b[0m             cast(JVMView, sc\u001b[38;5;241m.\u001b[39m_jvm)\u001b[38;5;241m.\u001b[39morg\u001b[38;5;241m.\u001b[39mapache\u001b[38;5;241m.\u001b[39mspark\u001b[38;5;241m.\u001b[39msql\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mStructType\u001b[38;5;241m.\u001b[39mfromDDL(type_str)\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m   1298\u001b[0m         )\n",
      "File \u001b[0;32m/home/vuphamas/code/fx/venv/lib/python3.10/site-packages/pyspark/sql/utils.py:248\u001b[0m, in \u001b[0;36mget_active_spark_context\u001b[0;34m()\u001b[0m\n\u001b[1;32m    246\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkContext or SparkSession should be created first.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\n",
      "\u001b[0;31mRuntimeError\u001b[0m: SparkContext or SparkSession should be created first."
     ]
    }
   ],
   "source": [
    "class ImprovedSparkGNN:\n",
    "    def __init__(self, input_dir, num_features=166, num_classes=2):\n",
    "        # Create Spark session with error handling\n",
    "        self.spark = sparkInit\n",
    "        \n",
    "        # Validate SparkSession\n",
    "        if not self.spark:\n",
    "            raise RuntimeError(\"Failed to initialize SparkSession.\")\n",
    "        \n",
    "        self.input_dir = input_dir\n",
    "        self.num_features = num_features\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Detect and set device\n",
    "        self.device = self._get_device()\n",
    "        \n",
    "        # Set random seeds for reproducibility\n",
    "        self._set_seeds()\n",
    "    \n",
    "    def _get_device(self):\n",
    "        \"\"\"\n",
    "        Detect and return appropriate device (GPU if available, else CPU)\n",
    "        \"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            print(\"Using CUDA GPU\")\n",
    "            return torch.device('cuda')\n",
    "        elif torch.backends.mps.is_available():\n",
    "            print(\"Using Apple Metal Performance Shaders\")\n",
    "            return torch.device('mps')\n",
    "        else:\n",
    "            print(\"Using CPU\")\n",
    "            return torch.device('cpu')\n",
    "    \n",
    "    def _set_seeds(self):\n",
    "        \"\"\"\n",
    "        Set random seeds for reproducibility\n",
    "        \"\"\"\n",
    "        torch.manual_seed(42)\n",
    "        np.random.seed(42)\n",
    "        if self.device.type == 'cuda':\n",
    "            torch.cuda.manual_seed_all(42)\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load data with more robust error handling\n",
    "        \"\"\"\n",
    "        try:\n",
    "            features_path = os.path.join(self.input_dir, 'elliptic_txs_features.csv')\n",
    "            classes_path = os.path.join(self.input_dir, 'elliptic_txs_classes.csv')\n",
    "            edgelist_path = os.path.join(self.input_dir, 'elliptic_txs_edgelist.csv')\n",
    "            \n",
    "            # Check if files exist\n",
    "            for path in [features_path, classes_path, edgelist_path]:\n",
    "                if not os.path.exists(path):\n",
    "                    raise FileNotFoundError(f\"File not found: {path}\")\n",
    "            \n",
    "            features_df = self.spark.read.csv(\n",
    "                features_path, \n",
    "                header=False, \n",
    "                inferSchema=True\n",
    "            )\n",
    "            \n",
    "            classes_df = self.spark.read.csv(\n",
    "                classes_path, \n",
    "                header=True, \n",
    "                inferSchema=True\n",
    "            )\n",
    "            \n",
    "            edgelist_df = self.spark.read.csv(\n",
    "                edgelist_path, \n",
    "                header=True, \n",
    "                inferSchema=True\n",
    "            )\n",
    "            \n",
    "            # Rename and process columns efficiently\n",
    "            features_cols = ['txId'] + [f'V{i}' for i in range(1, self.num_features + 1)]\n",
    "            features_df = features_df.toDF(*features_cols)\n",
    "            \n",
    "            # Use broadcast join for small dataframes\n",
    "            features_df = features_df.join(\n",
    "                classes_df.withColumn('class_mapped', \n",
    "                    F.when(F.col('class') == '1', 'illicit')\n",
    "                    .when(F.col('class') == '2', 'licit')\n",
    "                    .otherwise('unknown')\n",
    "                ), \n",
    "                'txId'\n",
    "            )\n",
    "            \n",
    "            return features_df, edgelist_df\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data: {e}\")\n",
    "            raise\n",
    "    \n",
    "    @pandas_udf(\"array<float>\", PandasUDFType.GROUPED_MAP)\n",
    "    def feature_engineering_udf(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Pandas UDF for feature engineering and normalization\n",
    "        \"\"\"\n",
    "        # Dynamically determine feature columns\n",
    "        feature_cols = [col for col in pdf.columns if col.startswith('V')]\n",
    "        \n",
    "        # Min-max scaling\n",
    "        pdf[feature_cols] = (pdf[feature_cols] - pdf[feature_cols].min()) / (pdf[feature_cols].max() - pdf[feature_cols].min())\n",
    "        \n",
    "        # Add synthetic feature\n",
    "        pdf['total_feature_sum'] = pdf[feature_cols].sum(axis=1)\n",
    "        \n",
    "        # Return processed dataframe\n",
    "        return pdf[feature_cols + ['total_feature_sum', 'txId', 'class_mapped']]\n",
    "    \n",
    "    def preprocess_data(self, features_df):\n",
    "        # Apply feature engineering UDF\n",
    "        engineered_df = features_df.groupBy('txId').apply(feature_engineering_udf)\n",
    "        \n",
    "        # Rest of the method remains the same\n",
    "        @pandas_udf(ArrayType(FloatType()), PandasUDFType.GROUPED_MAP)\n",
    "        def vectorize_features(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "            feature_cols = [f'V{i}' for i in range(1, 167)] + ['total_feature_sum']\n",
    "            pdf['features'] = pdf[feature_cols].apply(lambda row: Vectors.dense(row.values), axis=1)\n",
    "            return pdf[['txId', 'features', 'class_mapped']]\n",
    "        \n",
    "        vectorized_df = engineered_df.groupBy('txId').apply(vectorize_features)\n",
    "        \n",
    "        # String Indexing for labels\n",
    "        indexer = StringIndexer(inputCol='class_mapped', outputCol='label')\n",
    "        pipeline = Pipeline(stages=[indexer])\n",
    "        model = pipeline.fit(vectorized_df)\n",
    "        processed_df = model.transform(vectorized_df)\n",
    "        \n",
    "        return processed_df\n",
    "    \n",
    "    def create_torch_geometric_data(self, processed_df, edgelist_df):\n",
    "        # Convert Spark DataFrame to Pandas for efficient processing\n",
    "        processed_pdf = processed_df.toPandas()\n",
    "        edge_pdf = edgelist_df.toPandas()\n",
    "        \n",
    "        # Create node ID mapping\n",
    "        tx_id_mapping = {txid: idx for idx, txid in enumerate(processed_pdf['txId'])}\n",
    "        \n",
    "        # Prepare node features and labels\n",
    "        node_features = torch.tensor(\n",
    "            processed_pdf['features'].apply(lambda x: x.toArray()).values, \n",
    "            dtype=torch.float\n",
    "        )\n",
    "        \n",
    "        node_labels = torch.tensor(\n",
    "            processed_pdf['label'].values, \n",
    "            dtype=torch.long\n",
    "        )\n",
    "        \n",
    "        # Filter and map edge indices\n",
    "        edge_df_filtered = edge_pdf[\n",
    "            edge_pdf['txId1'].isin(processed_pdf['txId']) & \n",
    "            edge_pdf['txId2'].isin(processed_pdf['txId'])\n",
    "        ]\n",
    "        \n",
    "        edge_index_list = [\n",
    "            (tx_id_mapping[row['txId1']], tx_id_mapping[row['txId2']]) \n",
    "            for _, row in edge_df_filtered.iterrows()\n",
    "        ]\n",
    "        \n",
    "        edge_index = torch.tensor(edge_index_list, dtype=torch.long).t().contiguous()\n",
    "        \n",
    "        # Prepare graph data with train/val/test splits\n",
    "        data = Data(x=node_features, edge_index=edge_index, y=node_labels)\n",
    "        \n",
    "        num_nodes = data.num_nodes\n",
    "        train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        \n",
    "        known_mask = (data.y == 0) | (data.y == 1)\n",
    "        known_indices = torch.nonzero(known_mask).squeeze()\n",
    "        \n",
    "        perm = torch.randperm(len(known_indices))\n",
    "        known_indices = known_indices[perm]\n",
    "        \n",
    "        train_ratio, val_ratio = 0.8, 0.1\n",
    "        train_size = int(train_ratio * len(known_indices))\n",
    "        val_size = int(val_ratio * len(known_indices))\n",
    "        \n",
    "        train_indices = known_indices[:train_size]\n",
    "        val_indices = known_indices[train_size:train_size+val_size]\n",
    "        test_indices = known_indices[train_size+val_size:]\n",
    "        \n",
    "        train_mask[train_indices] = True\n",
    "        val_mask[val_indices] = True\n",
    "        test_mask[test_indices] = True\n",
    "        \n",
    "        data.train_mask = train_mask\n",
    "        data.val_mask = val_mask\n",
    "        data.test_mask = test_mask\n",
    "        \n",
    "        return data.to(self.device)\n",
    "    \n",
    "    class GCN(torch.nn.Module):\n",
    "        def __init__(self, num_features, num_classes):\n",
    "            super().__init__()\n",
    "            self.conv1 = GCNConv(num_features, 16)\n",
    "            self.conv2 = GCNConv(16, num_classes)\n",
    "            self.dropout = torch.nn.Dropout(0.5)\n",
    "        \n",
    "        def forward(self, data):\n",
    "            x, edge_index = data.x, data.edge_index\n",
    "            x = self.conv1(x, edge_index)\n",
    "            x = torch.nn.functional.relu(x)\n",
    "            x = self.dropout(x)\n",
    "            x = self.conv2(x, edge_index)\n",
    "            return torch.nn.functional.log_softmax(x, dim=1)\n",
    "    \n",
    "    def train(self, data, num_epochs=100):\n",
    "        # MLflow experiment tracking\n",
    "        mlflow.set_experiment(\"Transaction_Classification_GNN\")\n",
    "        \n",
    "        with mlflow.start_run():\n",
    "            # Log hyperparameters\n",
    "            mlflow.log_params({\n",
    "                \"num_features\": data.num_features,\n",
    "                \"num_classes\": self.num_classes,\n",
    "                \"learning_rate\": 0.01,\n",
    "                \"weight_decay\": 0.0005,\n",
    "                \"epochs\": num_epochs\n",
    "            })\n",
    "            \n",
    "            model = self.GCN(num_features=data.num_features, num_classes=self.num_classes).to(self.device)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0005)\n",
    "            criterion = torch.nn.CrossEntropyLoss()\n",
    "            \n",
    "            model.train()\n",
    "            for epoch in range(num_epochs):\n",
    "                optimizer.zero_grad()\n",
    "                out = model(data)\n",
    "                loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Log training loss\n",
    "                mlflow.log_metric(\"training_loss\", loss.item(), step=epoch)\n",
    "                \n",
    "                if epoch % 10 == 0:\n",
    "                    print(f'Epoch {epoch:03d}, Loss: {loss.item():.4f}')\n",
    "            \n",
    "            # Log model\n",
    "            mlflow.pytorch.log_model(model, \"gnn_model\")\n",
    "            \n",
    "        return model\n",
    "    \n",
    "    def evaluate(self, model, data):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = model(data)\n",
    "            pred = out.argmax(dim=1)\n",
    "\n",
    "            test_mask = data.test_mask\n",
    "            y_true = data.y[test_mask].cpu().numpy()\n",
    "            y_pred = pred[test_mask].cpu().numpy()\n",
    "            \n",
    "            precision = precision_score(y_true, y_pred, average='weighted')\n",
    "            recall = recall_score(y_true, y_pred, average='weighted')\n",
    "            f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "            \n",
    "            # Log evaluation metrics\n",
    "            mlflow.log_metrics({\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"f1_score\": f1\n",
    "            })\n",
    "            \n",
    "            print(\"\\nClassification Report:\")\n",
    "            print(classification_report(y_true, y_pred, target_names=['illicit', 'licit']))\n",
    "            \n",
    "            return {\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1_score': f1\n",
    "            }\n",
    "    \n",
    "    def run_pipeline(self, mlflow_tracking=False):\n",
    "        \"\"\"\n",
    "        Simplified pipeline method with optional MLflow tracking\n",
    "        \"\"\"\n",
    "        mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "        try:\n",
    "            # Optional MLflow tracking\n",
    "            if mlflow_tracking:\n",
    "                mlflow.set_experiment(\"Transaction_Classification_GNN\")\n",
    "                mlflow.start_run()\n",
    "            \n",
    "            # Load and preprocess data\n",
    "            features_df, edgelist_df = self.load_data()\n",
    "            \n",
    "            # Convert numeric columns to double\n",
    "            numeric_cols = [f'V{i}' for i in range(1, self.num_features + 1)]\n",
    "            for column in numeric_cols:\n",
    "                features_df = features_df.withColumn(column, col(column).cast(DoubleType()))\n",
    "            \n",
    "            print(\"Data loaded successfully. Starting preprocessing...\")\n",
    "            \n",
    "            # Preprocess data\n",
    "            processed_df = self.preprocess_data(features_df)\n",
    "            \n",
    "            # Create graph data\n",
    "            graph_data = self.create_torch_geometric_data(processed_df, edgelist_df)\n",
    "            \n",
    "            # Train model and log with MLflow\n",
    "            model = self.train(graph_data)\n",
    "            \n",
    "            # Evaluate model and log metrics\n",
    "            metrics = self.evaluate(model, graph_data)\n",
    "            \n",
    "            return model, metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in pipeline execution: {e}\")\n",
    "            raise\n",
    "        \n",
    "        finally:\n",
    "            # Always stop the Spark session\n",
    "            if self.spark:\n",
    "                self.spark.stop()\n",
    "            \n",
    "            # End MLflow run if started\n",
    "            if mlflow_tracking:\n",
    "                mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully. Starting preprocessing...\n",
      "Error in pipeline execution: Invalid udf: the udf argument must be a pandas_udf of type GROUPED_MAP.\n",
      "Pipeline failed: Invalid udf: the udf argument must be a pandas_udf of type GROUPED_MAP.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    spark = create_spark_session()\n",
    "    input_directory = ''\n",
    "    \n",
    "    try:\n",
    "        # Create and run the pipeline\n",
    "        gnn_pipeline = ImprovedSparkGNN(input_directory)\n",
    "        model, metrics = gnn_pipeline.run_pipeline()\n",
    "        print(\"\\nFinal Metrics:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Pipeline failed: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
