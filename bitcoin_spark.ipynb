{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/12/03 13:41:10 WARN Utils: Your hostname, KrystalXPS resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "24/12/03 13:41:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/03 13:41:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/12/03 13:41:57 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 10:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "# import torch.nn.functional as F\n",
    "from pyspark.sql import functions as F\n",
    "import torch_geometric\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from torch import Tensor\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TransactionClassificationGNN\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", \"100\")\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "class SparkGNN:\n",
    "    def __init__(self, input_dir, num_features=166, num_classes=2):\n",
    "        self.input_dir = input_dir\n",
    "        self.num_features = num_features\n",
    "        self.num_classes = num_classes\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "    def load_data(self):\n",
    "        features_path = os.path.join(self.input_dir, 'elliptic_txs_features.csv')\n",
    "        classes_path = os.path.join(self.input_dir, 'elliptic_txs_classes.csv')\n",
    "        edgelist_path = os.path.join(self.input_dir, 'elliptic_txs_edgelist.csv')\n",
    "        \n",
    "        features_df = spark.read.csv(features_path, header=False)\n",
    "        classes_df = spark.read.csv(classes_path, header=True)\n",
    "        edgelist_df = spark.read.csv(edgelist_path, header=True)\n",
    "        \n",
    "        features_cols = ['txId'] + [f'V{i}' for i in range(1, self.num_features + 1)]\n",
    "        features_df = features_df.toDF(*features_cols)\n",
    "        \n",
    "        classes_df = classes_df.withColumn('class_mapped', \n",
    "            F.when(F.col('class') == '1', 'illicit')\n",
    "            .when(F.col('class') == '2', 'licit')\n",
    "            .otherwise('unknown')\n",
    "        )\n",
    "        \n",
    "        return features_df, classes_df, edgelist_df\n",
    "    \n",
    "    def preprocess_data(self, features_df, classes_df):\n",
    "        feature_cols = [f'V{i}' for i in range(1, self.num_features + 1)]\n",
    "        assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "        indexer = StringIndexer(inputCol='class_mapped', outputCol='label')\n",
    "        pipeline = Pipeline(stages=[assembler, indexer])\n",
    "        preprocessor = pipeline.fit(features_df.join(classes_df, 'txId'))\n",
    "        processed_df = preprocessor.transform(features_df.join(classes_df, 'txId'))\n",
    "        \n",
    "        return processed_df\n",
    "\n",
    "    \n",
    "    def create_graph_data(self, processed_df, edgelist_df):\n",
    "        tx_id_mapping = {row['txId']: idx for idx, row in enumerate(processed_df.collect())}\n",
    "        \n",
    "        node_features = torch.tensor(\n",
    "            processed_df.select('features').rdd.map(lambda x: x['features'].toArray()).collect(), \n",
    "            dtype=torch.float\n",
    "        )\n",
    "\n",
    "        node_labels = torch.tensor(\n",
    "            processed_df.select('label').rdd.map(lambda x: x['label']).collect(), \n",
    "            dtype=torch.long\n",
    "        )\n",
    "        \n",
    "        edge_df = edgelist_df.filter(\n",
    "            F.col('txId1').isin(list(tx_id_mapping.keys())) & \n",
    "            F.col('txId2').isin(list(tx_id_mapping.keys()))\n",
    "        )\n",
    "        \n",
    "        edge_index_list = [\n",
    "            (tx_id_mapping[row['txId1']], tx_id_mapping[row['txId2']]) \n",
    "            for row in edge_df.collect()\n",
    "        ]\n",
    "        \n",
    "        edge_index = torch.tensor(edge_index_list, dtype=torch.long).t().contiguous()\n",
    "        \n",
    "        data = Data(x=node_features, edge_index=edge_index, y=node_labels)\n",
    "    \n",
    "        num_nodes = data.num_nodes\n",
    "        train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        \n",
    "        known_mask = (data.y == 0) | (data.y == 1)\n",
    "        known_indices = torch.nonzero(known_mask).squeeze()\n",
    "        \n",
    "        perm = torch.randperm(len(known_indices))\n",
    "        known_indices = known_indices[perm]\n",
    "        \n",
    "        train_ratio, val_ratio = 0.8, 0.1\n",
    "        train_size = int(train_ratio * len(known_indices))\n",
    "        val_size = int(val_ratio * len(known_indices))\n",
    "        \n",
    "        train_indices = known_indices[:train_size]\n",
    "        val_indices = known_indices[train_size:train_size+val_size]\n",
    "        test_indices = known_indices[train_size+val_size:]\n",
    "        \n",
    "        train_mask[train_indices] = True\n",
    "        val_mask[val_indices] = True\n",
    "        test_mask[test_indices] = True\n",
    "        \n",
    "        data.train_mask = train_mask\n",
    "        data.val_mask = val_mask\n",
    "        data.test_mask = test_mask\n",
    "        \n",
    "        return data.to(self.device)\n",
    "    \n",
    "    class GCN(torch.nn.Module):\n",
    "        def __init__(self, num_features, num_classes):\n",
    "            super().__init__()\n",
    "            self.conv1 = GCNConv(num_features, 16)\n",
    "            self.conv2 = GCNConv(16, num_classes)\n",
    "        \n",
    "        def forward(self, data):\n",
    "            x, edge_index = data.x, data.edge_index\n",
    "            x = self.conv1(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = self.conv2(x, edge_index)\n",
    "            return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def train(self, data, num_epochs=100):\n",
    "        model = self.GCN(num_features=data.num_features, num_classes=self.num_classes).to(self.device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0005)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data)\n",
    "            loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch {epoch:03d}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def evaluate(self, model, data):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = model(data)\n",
    "            pred = out.argmax(dim=1)\n",
    "\n",
    "            test_mask = data.test_mask\n",
    "            y_true = data.y[test_mask].cpu().numpy()\n",
    "            y_pred = pred[test_mask].cpu().numpy()\n",
    "            \n",
    "            precision = precision_score(y_true, y_pred, average='weighted')\n",
    "            recall = recall_score(y_true, y_pred, average='weighted')\n",
    "            f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "            \n",
    "            print(\"\\nClassification Report:\")\n",
    "            print(classification_report(y_true, y_pred, target_names=['illicit', 'licit']))\n",
    "            \n",
    "            return {\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1_score': f1\n",
    "            }\n",
    "    \n",
    "    def run_pipeline(self):\n",
    "        features_df, classes_df, edgelist_df = self.load_data()\n",
    "        numeric_cols = [col_name for col_name in features_df.columns if col_name != 'txId']\n",
    "        for column in numeric_cols:\n",
    "            features_df = features_df.withColumn(column, col(column).cast(DoubleType()))\n",
    "        \n",
    "        processed_df = self.preprocess_data(features_df, classes_df)\n",
    "        \n",
    "        graph_data = self.create_graph_data(processed_df, edgelist_df)\n",
    "        \n",
    "        model = self.train(graph_data)\n",
    "        \n",
    "        metrics = self.evaluate(model, graph_data)\n",
    "        \n",
    "        return model, metrics\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_directory = ''\n",
    "    gnn_pipeline = SparkGNN(input_directory)\n",
    "    model, metrics = gnn_pipeline.run_pipeline()\n",
    "    print(\"\\nFinal Metrics:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "\n",
    "    spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
