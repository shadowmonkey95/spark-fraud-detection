{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "# import torch.nn.functional as F\n",
    "from pyspark.sql import functions as F\n",
    "import torch_geometric\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from torch import Tensor\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TransactionClassificationGNN\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", \"100\")\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "class SparkGNN:\n",
    "    def __init__(self, input_dir, num_features=166, num_classes=2):\n",
    "        \"\"\"\n",
    "        Initialize the Spark-PyTorch Geometric Pipeline\n",
    "        \n",
    "        :param input_dir: Directory containing input CSV files\n",
    "        :param num_features: Number of input features\n",
    "        :param num_classes: Number of output classes\n",
    "        \"\"\"\n",
    "        self.input_dir = input_dir\n",
    "        self.num_features = num_features\n",
    "        self.num_classes = num_classes\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load data using PySpark\n",
    "        \n",
    "        :return: Tuple of features, classes, and edgelist DataFrames\n",
    "        \"\"\"\n",
    "        # Load CSV files\n",
    "        features_path = os.path.join(self.input_dir, 'elliptic_txs_features.csv')\n",
    "        classes_path = os.path.join(self.input_dir, 'elliptic_txs_classes.csv')\n",
    "        edgelist_path = os.path.join(self.input_dir, 'elliptic_txs_edgelist.csv')\n",
    "        \n",
    "        # Read as Spark DataFrames\n",
    "        features_df = spark.read.csv(features_path, header=False)\n",
    "        classes_df = spark.read.csv(classes_path, header=True)\n",
    "        edgelist_df = spark.read.csv(edgelist_path, header=True)\n",
    "        \n",
    "        # Rename and prepare features columns\n",
    "        features_cols = ['txId'] + [f'V{i}' for i in range(1, self.num_features + 1)]\n",
    "        features_df = features_df.toDF(*features_cols)\n",
    "        \n",
    "        # Map classes\n",
    "        classes_df = classes_df.withColumn('class_mapped', \n",
    "            F.when(F.col('class') == '1', 'illicit')\n",
    "            .when(F.col('class') == '2', 'licit')\n",
    "            .otherwise('unknown')\n",
    "        )\n",
    "        \n",
    "        return features_df, classes_df, edgelist_df\n",
    "    \n",
    "    def preprocess_data(self, features_df, classes_df):\n",
    "        \"\"\"\n",
    "        Preprocess data using PySpark ML Pipeline\n",
    "        \n",
    "        :param features_df: Features DataFrame\n",
    "        :param classes_df: Classes DataFrame\n",
    "        :return: Preprocessed DataFrame\n",
    "        \"\"\"\n",
    "        # Prepare feature columns for vector assembly\n",
    "        feature_cols = [f'V{i}' for i in range(1, self.num_features + 1)]\n",
    "        \n",
    "        # Vector Assembler to combine features\n",
    "        assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "        \n",
    "        # String Indexer for target variable\n",
    "        indexer = StringIndexer(inputCol='class_mapped', outputCol='label')\n",
    "        \n",
    "        # Create ML Pipeline\n",
    "        pipeline = Pipeline(stages=[assembler, indexer])\n",
    "        \n",
    "        # Fit and transform data\n",
    "        preprocessor = pipeline.fit(features_df.join(classes_df, 'txId'))\n",
    "        processed_df = preprocessor.transform(features_df.join(classes_df, 'txId'))\n",
    "        \n",
    "        return processed_df\n",
    "\n",
    "    \n",
    "    def create_graph_data(self, processed_df, edgelist_df):\n",
    "        \"\"\"\n",
    "        Create PyTorch Geometric graph data\n",
    "        \n",
    "        :param processed_df: Preprocessed DataFrame\n",
    "        :param edgelist_df: Edgelist DataFrame\n",
    "        :return: PyTorch Geometric Data object\n",
    "        \"\"\"\n",
    "        # Create tx_id to index mapping\n",
    "        tx_id_mapping = {row['txId']: idx for idx, row in enumerate(processed_df.collect())}\n",
    "        \n",
    "        # Prepare node features\n",
    "        node_features = torch.tensor(\n",
    "            processed_df.select('features').rdd.map(lambda x: x['features'].toArray()).collect(), \n",
    "            dtype=torch.float\n",
    "        )\n",
    "        \n",
    "        # Prepare node labels\n",
    "        node_labels = torch.tensor(\n",
    "            processed_df.select('label').rdd.map(lambda x: x['label']).collect(), \n",
    "            dtype=torch.long\n",
    "        )\n",
    "        \n",
    "        # Prepare edge index\n",
    "        edge_df = edgelist_df.filter(\n",
    "            F.col('txId1').isin(list(tx_id_mapping.keys())) & \n",
    "            F.col('txId2').isin(list(tx_id_mapping.keys()))\n",
    "        )\n",
    "        \n",
    "        edge_index_list = [\n",
    "            (tx_id_mapping[row['txId1']], tx_id_mapping[row['txId2']]) \n",
    "            for row in edge_df.collect()\n",
    "        ]\n",
    "        \n",
    "        edge_index = torch.tensor(edge_index_list, dtype=torch.long).t().contiguous()\n",
    "        \n",
    "        # Create PyTorch Geometric Data object\n",
    "        data = Data(x=node_features, edge_index=edge_index, y=node_labels)\n",
    "        \n",
    "        # Split data\n",
    "        num_nodes = data.num_nodes\n",
    "        train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        \n",
    "        # Randomly split known nodes\n",
    "        known_mask = (data.y == 0) | (data.y == 1)\n",
    "        known_indices = torch.nonzero(known_mask).squeeze()\n",
    "        \n",
    "        # Shuffle known indices\n",
    "        perm = torch.randperm(len(known_indices))\n",
    "        known_indices = known_indices[perm]\n",
    "        \n",
    "        # Split ratios\n",
    "        train_ratio, val_ratio = 0.8, 0.1\n",
    "        train_size = int(train_ratio * len(known_indices))\n",
    "        val_size = int(val_ratio * len(known_indices))\n",
    "        \n",
    "        # Create masks\n",
    "        train_indices = known_indices[:train_size]\n",
    "        val_indices = known_indices[train_size:train_size+val_size]\n",
    "        test_indices = known_indices[train_size+val_size:]\n",
    "        \n",
    "        train_mask[train_indices] = True\n",
    "        val_mask[val_indices] = True\n",
    "        test_mask[test_indices] = True\n",
    "        \n",
    "        data.train_mask = train_mask\n",
    "        data.val_mask = val_mask\n",
    "        data.test_mask = test_mask\n",
    "        \n",
    "        return data.to(self.device)\n",
    "    \n",
    "    class GCN(torch.nn.Module):\n",
    "        def __init__(self, num_features, num_classes):\n",
    "            super().__init__()\n",
    "            self.conv1 = GCNConv(num_features, 16)\n",
    "            self.conv2 = GCNConv(16, num_classes)\n",
    "        \n",
    "        def forward(self, data):\n",
    "            x, edge_index = data.x, data.edge_index\n",
    "            x = self.conv1(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = self.conv2(x, edge_index)\n",
    "            return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def train(self, data, num_epochs=100):\n",
    "        \"\"\"\n",
    "        Train the Graph Neural Network\n",
    "        \n",
    "        :param data: PyTorch Geometric Data object\n",
    "        :param num_epochs: Number of training epochs\n",
    "        :return: Trained model and training metrics\n",
    "        \"\"\"\n",
    "        # Initialize model\n",
    "        model = self.GCN(num_features=data.num_features, num_classes=self.num_classes).to(self.device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0005)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Training loop\n",
    "        model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data)\n",
    "            loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Print progress\n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch {epoch:03d}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def evaluate(self, model, data):\n",
    "        \"\"\"\n",
    "        Evaluate the model\n",
    "        \n",
    "        :param model: Trained PyTorch Geometric model\n",
    "        :param data: PyTorch Geometric Data object\n",
    "        :return: Evaluation metrics\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = model(data)\n",
    "            pred = out.argmax(dim=1)\n",
    "            \n",
    "            # Metrics for test set\n",
    "            test_mask = data.test_mask\n",
    "            y_true = data.y[test_mask].cpu().numpy()\n",
    "            y_pred = pred[test_mask].cpu().numpy()\n",
    "            \n",
    "            # Calculate metrics\n",
    "            precision = precision_score(y_true, y_pred, average='weighted')\n",
    "            recall = recall_score(y_true, y_pred, average='weighted')\n",
    "            f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "            \n",
    "            # Print classification report\n",
    "            print(\"\\nClassification Report:\")\n",
    "            print(classification_report(y_true, y_pred, target_names=['illicit', 'licit']))\n",
    "            \n",
    "            return {\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1_score': f1\n",
    "            }\n",
    "    \n",
    "    def run_pipeline(self):\n",
    "        \"\"\"\n",
    "        Run full ML pipeline\n",
    "        \"\"\"\n",
    "        # Load data\n",
    "        features_df, classes_df, edgelist_df = self.load_data()\n",
    "            # Convert string columns to numeric\n",
    "        # def convert_to_numeric(df):\n",
    "        #     # List of columns to convert\n",
    "        #     numeric_cols = ['V162', 'V163', 'V164', 'V165', 'V166']\n",
    "            \n",
    "        #     # Convert each column to double\n",
    "        #     for column in numeric_cols:\n",
    "        #         df = df.withColumn(column, col(column).cast(DoubleType()))\n",
    "            \n",
    "        #     return df\n",
    "        \n",
    "        # Before creating the pipeline, convert columns\n",
    "        numeric_cols = [col_name for col_name in features_df.columns if col_name != 'txId']\n",
    "        for column in numeric_cols:\n",
    "            features_df = features_df.withColumn(column, col(column).cast(DoubleType()))\n",
    "        \n",
    "        # Preprocess data\n",
    "        processed_df = self.preprocess_data(features_df, classes_df)\n",
    "        \n",
    "        # Create graph data\n",
    "        graph_data = self.create_graph_data(processed_df, edgelist_df)\n",
    "        \n",
    "        # Train model\n",
    "        model = self.train(graph_data)\n",
    "        \n",
    "        # Evaluate model\n",
    "        metrics = self.evaluate(model, graph_data)\n",
    "        \n",
    "        return model, metrics\n",
    "\n",
    "# Main execution\n",
    "if __name__ == '__main__':\n",
    "    # Specify input directory containing CSV files\n",
    "    input_directory = ''\n",
    "    \n",
    "    # Initialize and run pipeline\n",
    "    gnn_pipeline = SparkGNN(input_directory)\n",
    "    model, metrics = gnn_pipeline.run_pipeline()\n",
    "    \n",
    "    # Print final metrics\n",
    "    print(\"\\nFinal Metrics:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "    \n",
    "    # Close Spark session\n",
    "    spark.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
